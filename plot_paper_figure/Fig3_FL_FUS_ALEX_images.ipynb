{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tifffile import imread\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_FUS_FL = \"/Volumes/lsa-nwalter/Guoming_Gao_turbo/Walterlab_server/PROCESSED_DATA/RNA-diffusion-in-FUS/RNAinFUS_PaperFigures/Fig3_coralled by nano domains/FUS488_FL_PAINT\"\n",
    "os.chdir(folder_FUS_FL)\n",
    "\n",
    "switch_perLoc_perTrack = \"perLoc\"\n",
    "# switch_perLoc_perTrack = \"perTrack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following parameters are the same for all PairCorr-DataDict.p\n",
    "if switch_perLoc_perTrack == \"perLoc\":\n",
    "    dict_toplot = pickle.load(open(\"PairCorr-DataDict-pooled-perLoc.p\", \"rb\"))\n",
    "elif switch_perLoc_perTrack == \"perTrack\":\n",
    "    dict_toplot = pickle.load(open(\"PairCorr-DataDict-pooled-perTrack.p\", \"rb\"))\n",
    "\n",
    "nm_per_pxl = dict_toplot[\"nm_per_pxl\"]\n",
    "r_max_nm = dict_toplot[\"r_max_nm\"]\n",
    "ringwidth_nm = dict_toplot[\"ringwidth_nm\"]\n",
    "dr_slidingrings_nm = dict_toplot[\"dr_slidingrings_nm\"]\n",
    "bins = dict_toplot[\"bins\"]\n",
    "dict_toplot.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cnt2mask(imgshape, contours):\n",
    "    # create empty image\n",
    "    mask = np.zeros(imgshape, dtype=np.uint8)\n",
    "    # draw contour\n",
    "    for cnt in contours:\n",
    "        cv2.fillPoly(mask, [cnt], (255))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def single_condensate_stepsize_img(df_track, img_shape):\n",
    "    ## Reconstruct step size iamge, unit: um\n",
    "    lst_mid_x = []\n",
    "    lst_mid_y = []\n",
    "    lst_stepsize = []\n",
    "    all_trackID = df_track[\"trackID\"].unique()\n",
    "    for trackID in all_trackID:\n",
    "        df_current = df_track[df_track[\"trackID\"] == trackID]\n",
    "        xs = df_current[\"x\"].to_numpy(float)\n",
    "        ys = df_current[\"y\"].to_numpy(float)\n",
    "        mid_xs = (xs[1:] + xs[:-1]) / 2\n",
    "        mid_ys = (ys[1:] + ys[:-1]) / 2\n",
    "        steps = (\n",
    "            np.sqrt((xs[1:] - xs[:-1]) ** 2 + (ys[1:] - ys[:-1]) ** 2) * um_per_pixel\n",
    "        )\n",
    "        lst_mid_x.extend(mid_xs)\n",
    "        lst_mid_y.extend(mid_ys)\n",
    "        lst_stepsize.extend(steps)\n",
    "\n",
    "    df_all_steps = pd.DataFrame(\n",
    "        {\n",
    "            \"mid_x\": lst_mid_x,\n",
    "            \"mid_y\": lst_mid_y,\n",
    "            \"stepsize\": lst_stepsize,\n",
    "        },\n",
    "        dtype=float,\n",
    "    )\n",
    "\n",
    "    # put them in grid, calculate mean\n",
    "    img_stepsize = np.zeros(img_shape)\n",
    "    for x in range(img_stepsize.shape[0]):\n",
    "        for y in range(img_stepsize.shape[1]):\n",
    "            df_current = df_all_steps[\n",
    "                df_all_steps[\"mid_x\"].between(x, x + 1)\n",
    "                & df_all_steps[\"mid_y\"].between(y, y + 1)\n",
    "            ]\n",
    "            mean_stepsize = df_current[\"stepsize\"].mean()\n",
    "            img_stepsize[x, y] = mean_stepsize\n",
    "\n",
    "    return img_stepsize\n",
    "\n",
    "\n",
    "def corr_within_mask(df, mask):\n",
    "    \"\"\"\n",
    "    Take a Polygon mask and a dataframe contain columns 'x' and 'y', and return numpy array of x and y within the mask.\n",
    "    \"\"\"\n",
    "    lst_x = []\n",
    "    lst_y = []\n",
    "    for _, row in df.iterrows():\n",
    "        if Point(row.x, row.y).within(mask):\n",
    "            lst_x.append(row.x)\n",
    "            lst_y.append(row.y)\n",
    "    array_x = np.array(lst_x, dtype=float)\n",
    "    array_y = np.array(lst_y, dtype=float)\n",
    "    return array_x, array_y\n",
    "\n",
    "\n",
    "def filter_perLoc(df):\n",
    "    scaling_factor = 1\n",
    "    tracklength_threshold = 10\n",
    "    # single-frame spots\n",
    "    df_single_frame_spots = df[df[\"trackID\"].isna()]\n",
    "    spots_x = df_single_frame_spots.x.to_numpy(float)\n",
    "    spots_y = df_single_frame_spots.y.to_numpy(float)\n",
    "    # tracks\n",
    "    df_tracks = df[df[\"trackID\"].notna()]\n",
    "    all_trackID = df_tracks[\"trackID\"].unique()\n",
    "    lst_of_arr_x = []\n",
    "    lst_of_arr_y = []\n",
    "    for trackID in all_trackID:\n",
    "        df_current = df_tracks[df_tracks[\"trackID\"] == trackID]\n",
    "        # for short tracks, treat as spots\n",
    "        if df_current.shape[0] <= tracklength_threshold:\n",
    "            lst_of_arr_x.append(df_current[\"x\"].to_numpy(float) * scaling_factor)\n",
    "            lst_of_arr_y.append(df_current[\"y\"].to_numpy(float) * scaling_factor)\n",
    "            continue\n",
    "        # for long tracks, randomly pick tracklength_threshold number of spots\n",
    "        else:\n",
    "            chosen_idx = np.random.choice(df_current.shape[0], tracklength_threshold)\n",
    "            lst_of_arr_x.append(\n",
    "                df_current.iloc[chosen_idx][\"x\"].to_numpy(float) * scaling_factor\n",
    "            )\n",
    "            lst_of_arr_y.append(\n",
    "                df_current.iloc[chosen_idx][\"y\"].to_numpy(float) * scaling_factor\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    tracks_x = np.hstack(lst_of_arr_x)\n",
    "    tracks_y = np.hstack(lst_of_arr_y)\n",
    "\n",
    "    df_out = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": np.concatenate(\n",
    "                [\n",
    "                    spots_x,\n",
    "                    tracks_x,\n",
    "                ]\n",
    "            ),\n",
    "            \"y\": np.concatenate(\n",
    "                [\n",
    "                    spots_y,\n",
    "                    tracks_y,\n",
    "                ]\n",
    "            ),\n",
    "        },\n",
    "        dtype=float,\n",
    "    )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filter_perTrack(df):\n",
    "    scaling_factor = 1\n",
    "    tracklength_threshold = 3\n",
    "    df_tracks = df[df[\"trackID\"].notna()]\n",
    "    all_trackID = df_tracks[\"trackID\"].unique()\n",
    "    lst_x = []\n",
    "    lst_y = []\n",
    "    for trackID in all_trackID:\n",
    "        df_current = df_tracks[df_tracks[\"trackID\"] == trackID]\n",
    "        if df_current.shape[0] >= tracklength_threshold:\n",
    "            lst_x.append(df_current[\"x\"].mean() * scaling_factor)\n",
    "            lst_y.append(df_current[\"y\"].mean() * scaling_factor)\n",
    "\n",
    "    tracks_x = np.array(lst_x, dtype=float)\n",
    "    tracks_y = np.array(lst_y, dtype=float)\n",
    "\n",
    "    df_out = pd.DataFrame({\"x\": tracks_x, \"y\": tracks_y}, dtype=float)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def PairCorr_with_edge_correction(df_ref, df_interest, mask):\n",
    "    # only count particles within mask\n",
    "    x_ref, y_ref = corr_within_mask(df_ref, mask)\n",
    "    x_interest, y_interest = corr_within_mask(df_interest, mask)\n",
    "\n",
    "    # Total number particles in mask\n",
    "    N_ref = x_ref.shape[0]\n",
    "    N_interest = x_interest.shape[0]\n",
    "\n",
    "    # particle density rho, unit: number per nano meter square\n",
    "    mask_area_nm2 = mask.area * (nm_per_pxl**2)\n",
    "    rho_ref_per_nm2 = N_ref / mask_area_nm2\n",
    "    rho_interest_per_nm2 = N_interest / mask_area_nm2\n",
    "\n",
    "    # setup bins and ring areas\n",
    "    bin_starts = np.arange(0, r_max_nm - ringwidth_nm, dr_slidingrings_nm)\n",
    "    bin_ends = bin_starts + ringwidth_nm\n",
    "    ring_areas_nm2 = np.pi * (\n",
    "        bin_ends**2 - bin_starts**2\n",
    "    )  # area of rings, unit nm square\n",
    "    ring_areas_pxl2 = ring_areas_nm2 / (nm_per_pxl**2)\n",
    "\n",
    "    # Calculate corrected histogram of distances\n",
    "    lst_hist_per_point_cross = []\n",
    "    lst_hist_per_point_auto_ref = []\n",
    "    for i in range(len(x_ref)):\n",
    "        # Calculate edge correction factor\n",
    "        rings = [\n",
    "            Point(x_ref[i], y_ref[i])\n",
    "            .buffer(end)\n",
    "            .difference(Point(x_ref[i], y_ref[i]).buffer(start))\n",
    "            for start, end in zip(bin_starts / nm_per_pxl, bin_ends / nm_per_pxl)\n",
    "        ]\n",
    "        intersect_areas = np.array(\n",
    "            [mask.intersection(Polygon(ring), grid_size=0.1).area for ring in rings]\n",
    "        )\n",
    "        edge_correction_factors = 1 / (intersect_areas / ring_areas_pxl2)\n",
    "\n",
    "        # cross correlation\n",
    "        lst_hist = []\n",
    "        for j in range(len(x_interest)):\n",
    "            distance = (\n",
    "                np.sqrt(\n",
    "                    (x_ref[i] - x_interest[j]) ** 2 + (y_ref[i] - y_interest[j]) ** 2\n",
    "                )\n",
    "                * nm_per_pxl\n",
    "            )\n",
    "            lst_hist.append(((bin_starts <= distance) & (bin_ends >= distance)) * 1)\n",
    "        hist_per_point_corrected = np.sum(lst_hist, axis=0) * edge_correction_factors\n",
    "        lst_hist_per_point_cross.append(hist_per_point_corrected)\n",
    "\n",
    "        # auto correlation - ref\n",
    "        lst_hist = []\n",
    "        for j in range(len(x_ref)):\n",
    "            distance = (\n",
    "                np.sqrt((x_ref[i] - x_ref[j]) ** 2 + (y_ref[i] - y_ref[j]) ** 2)\n",
    "                * nm_per_pxl\n",
    "            )\n",
    "            lst_hist.append(((bin_starts <= distance) & (bin_ends >= distance)) * 1)\n",
    "        hist_per_point_corrected = np.sum(lst_hist, axis=0) * edge_correction_factors\n",
    "        lst_hist_per_point_auto_ref.append(hist_per_point_corrected)\n",
    "\n",
    "    # calculate normalization factor that counts for density and ring area\n",
    "    norm_factors_cross = N_ref * ring_areas_nm2 * rho_interest_per_nm2\n",
    "    norm_factors_auto_ref = N_ref * ring_areas_nm2 * rho_ref_per_nm2\n",
    "\n",
    "    PairCorr_cross = np.sum(lst_hist_per_point_cross, axis=0) / norm_factors_cross\n",
    "    PairCorr_auto_ref = (\n",
    "        np.sum(lst_hist_per_point_auto_ref, axis=0) / norm_factors_auto_ref\n",
    "    )\n",
    "\n",
    "    return PairCorr_cross, PairCorr_auto_ref\n",
    "\n",
    "\n",
    "def spots2PAINT_single_condensate(df, frame_size_x, frame_size_y):\n",
    "    # The whole df would be used for reconstructure, dispite track or spot\n",
    "    tracklength_threshold = 10\n",
    "    xedges = np.arange(frame_size_x + 1)\n",
    "    yedges = np.arange(frame_size_y + 1)\n",
    "    img_PAINT, _, _ = np.histogram2d(\n",
    "        x=df[\"x\"].to_numpy(float),\n",
    "        y=df[\"y\"].to_numpy(float),\n",
    "        bins=(xedges, yedges),\n",
    "    )\n",
    "    return img_PAINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    spots2PAINT_single_condensate(\n",
    "        df_left, img_PAINT_left.shape[1], img_PAINT_left.shape[0]\n",
    "    ),\n",
    "    cmap=\"Blues\",\n",
    "    vmin=0,\n",
    "    vmax=5,\n",
    ")\n",
    "x, y = mask_large.exterior.xy\n",
    "plt.plot(x, y, \"k\")\n",
    "# plt.savefig(\"21-PAINT_left.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autoFUS weighted mean, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    spots2PAINT_single_condensate(\n",
    "        df_right, img_PAINT_right.shape[1], img_PAINT_right.shape[0]\n",
    "    ),\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0,\n",
    "    vmax=5,\n",
    ")\n",
    "x, y = mask_large.exterior.xy\n",
    "plt.plot(x, y, \"k\")\n",
    "# plt.savefig(\"21-PAINT_right.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
